{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multitask-polynomial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjXo2WSLejRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import time\n",
        "import pylab\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import random\n",
        "\n",
        "#num_tasks = 1\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "def gen_data(N, num_tasks, sigma):\n",
        "    X = np.random.randn(N, 1)    \n",
        "    w = [random.choice(list(range(-5, 0)) + list(range(1, 6))) for i in range(num_tasks)]\n",
        "    b = [random.randint(-5, 5) for i in range(num_tasks)]\n",
        "    #w = [random.choice(list(range(-5, 0)) + list(range(1, 6))) for i in range(num_tasks)]\n",
        "    #b = [random.randint(-5, 5) for i in range(num_tasks)]\n",
        "    #sigma = [random.randint(1, 10) for i in range(num_tasks)]\n",
        "    Y = [0] * num_tasks\n",
        "    for i in range(num_tasks):\n",
        "        Y[i] = X.dot(w[i]) + b[i] + sigma[i] * np.random.randn(N, 1)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "def gen_val_data(N, num_tasks):\n",
        "    X = np.random.randn(N, 1)\n",
        "    #w = [random.choice(list(range(-5, 0)) + list(range(1, 6))) for i in range(num_tasks)]\n",
        "    #b = [random.randint(-5, 5) for i in range(num_tasks)]\n",
        "    #sigma = [random.randint(1, 10) for i in range(num_tasks)]\n",
        "    Y = [0] * num_tasks\n",
        "    for i in range(num_tasks):\n",
        "        Y[i] = X.dot(w[i]) + b[i] + sigma[i] * np.random.randn(N, 1)\n",
        "\n",
        "    return X, Y\n",
        "  \n",
        "def gen_polynomial_data(N, num_tasks, sigma, degree, regression=True):\n",
        "    X = np.random.randn(N, 1)\n",
        "    w = [[random.choice(list(range(-5, 0)) + list(range(1, 6))) for i in range(num_tasks)] for j in range(degree)]\n",
        "    b = [random.randint(-5, -5) for i in range(num_tasks)]\n",
        "    Y = [0] * num_tasks\n",
        "    for i in range(num_tasks):\n",
        "        Y[i] = b[i] + sigma[i] * np.random.randn(N, 1)\n",
        "        for j in range(1, degree+1):\n",
        "            Y[i] += (X**j).dot(w[j-1][i])\n",
        "        \n",
        "    return X, Y\n",
        "\n",
        "\n",
        "\n",
        "class Data(Dataset):\n",
        "\n",
        "    def __init__(self, feature_num, X, Y):\n",
        "        self.num_tasks = len(Y)\n",
        "        self.feature_num = feature_num\n",
        "\n",
        "        self.X = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "        self.Y = [0] * self.num_tasks\n",
        "        for i in range(self.num_tasks):\n",
        "            #self.Y[i] = torch.from_numpy(Y[i])\n",
        "            self.Y[i] = torch.tensor(Y[i], dtype=torch.float32, device=device)\n",
        "    def __len__(self):\n",
        "        return self.feature_num\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx,:], [self.Y[i][idx,:] for i in range(self.num_tasks)]\n",
        "\n",
        "class MultiTaskLossWrapper(nn.Module):\n",
        "    def __init__(self, num_tasks, model):\n",
        "        super(MultiTaskLossWrapper, self).__init__()\n",
        "        self.model = model\n",
        "        self.num_tasks = num_tasks\n",
        "        self.log_vars = nn.Parameter(torch.zeros((num_tasks), device=device))\n",
        "\n",
        "    def forward(self, input, targets):\n",
        "        outputs = self.model(input)\n",
        "        loss = 0\n",
        "        task_losses = [0] * self.num_tasks\n",
        "        precision = [0] * self.num_tasks\n",
        "        for i in range(self.num_tasks):\n",
        "            precision[i] = torch.exp(-self.log_vars[i])\n",
        "            task_loss = torch.sum(precision[i] * (targets[i] - outputs[i]) ** 2. + self.log_vars[i], -1)\n",
        "            task_losses[i] = torch.mean(task_loss).item()\n",
        "            loss += task_loss\n",
        "        \n",
        "        return torch.mean(loss), task_losses, self.log_vars.data.tolist()\n",
        "\n",
        "\n",
        "class MTLModel(torch.nn.Module):\n",
        "    def __init__(self, n_hidden, n_output, num_tasks):\n",
        "        super(MTLModel, self).__init__()\n",
        "        self.num_tasks = num_tasks\n",
        "        self.shared_fc = nn.Sequential(nn.Linear(1, n_hidden), nn.ReLU())\n",
        "        self.nets = [0] * num_tasks\n",
        "        \n",
        "        for i in range(num_tasks):\n",
        "            self.nets[i] = nn.Sequential(nn.Linear(n_hidden, n_hidden), nn.ReLU(), nn.Linear(n_hidden, n_output)).to(device)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        shared_out = self.shared_fc(x)\n",
        "        return [self.nets[i](shared_out) for i in range(self.num_tasks)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIE1Xjv_ep5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 5\n",
        "np.random.seed(seed)\n",
        "\n",
        "feature_num = 1500\n",
        "val_feature_num = 200\n",
        "nb_epoch = 2000\n",
        "batch_size = 75\n",
        "hidden_dim = 512\n",
        "lr = 0.1\n",
        "fixed = False\n",
        "fixed_sigma = 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWx6naizesQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import math\n",
        "torch.autograd.set_detect_anomaly(True) \n",
        "patience = 50\n",
        "delta = 1e-4\n",
        "num_iters = 5\n",
        "max_num_tasks = 5\n",
        "avg_min_val_loss = [0] * (max_num_tasks - 1)\n",
        "\n",
        "for num_tasks in range(2, max_num_tasks + 1):\n",
        "    for i in range(num_iters):\n",
        "        print(i)\n",
        "        if fixed:\n",
        "            sigma = [fixed_sigma for _ in range(num_tasks)]\n",
        "        else:\n",
        "            #sigma = [random.randint(1, 3) for i in range(num_tasks)]\n",
        "            sigma = list(range(1, num_tasks+1))\n",
        "        X, Y = gen_polynomial_data(feature_num + val_feature_num, num_tasks, sigma, 8)\n",
        "        X_val, Y_val = X[feature_num:], [y[feature_num:] for y in Y]\n",
        "        X, Y = X[:feature_num], [y[:feature_num] for y in Y]\n",
        "        \n",
        "        lowest_val_loss = None\n",
        "        counter = 0\n",
        "        early_stop = False\n",
        "\n",
        "\n",
        "        train_data = Data(feature_num, X, Y)\n",
        "        val_data = Data(val_feature_num, X_val, Y_val)\n",
        "        train_data_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "        val_data_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "        model = MTLModel(hidden_dim, 1, num_tasks)\n",
        "        mtl = MultiTaskLossWrapper(num_tasks, model)\n",
        "\n",
        "        model.to(device)\n",
        "        mtl.to(device)\n",
        "\n",
        "        # https://github.com/keras-team/keras/blob/master/keras/optimizers.py\n",
        "        # k.epsilon() = keras.backend.epsilon()\n",
        "        optimizer = torch.optim.Adam(mtl.parameters(), lr=lr, eps=1e-07)\n",
        "\n",
        "        loss_list = []\n",
        "        val_loss_list = []\n",
        "        plot_val_loss = []\n",
        "        times = []\n",
        "        for t in range(nb_epoch):\n",
        "            start = time.time()\n",
        "            cumulative_loss = 0\n",
        "            cumulative_val_loss = 0\n",
        "            cumulative_task_losses = [0] * num_tasks\n",
        "            cumulative_task_losses_val = [0] * num_tasks\n",
        "\n",
        "            for X_batch, Y_batch in train_data_loader:\n",
        "                X_batch, Y_batch = X_batch.to(device), [y.to(device) for y in Y_batch]\n",
        "\n",
        "                loss, task_losses, log_vars = mtl(X_batch, Y_batch)\n",
        "                cumulative_task_losses = [cumulative_task_losses[i] + task_losses[i] for i in range(len(cumulative_task_losses))]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                cumulative_loss += loss.item()\n",
        "            with torch.no_grad():\n",
        "                for X_val_batch, Y_val_batch in val_data_loader:\n",
        "                    X_val_batch, Y_val_batch = X_val_batch.to(device), [y.to(device) for y in Y_val_batch]\n",
        "                    val_loss, task_losses_val, _ = mtl(X_val_batch, Y_val_batch)\n",
        "                    cumulative_task_losses_val = [cumulative_task_losses_val[i] + task_losses_val[i] for i in range(len(cumulative_task_losses_val))]\n",
        "\n",
        "                    cumulative_val_loss += val_loss.item()\n",
        "\n",
        "            #loss_list.append(cumulative_loss/(feature_num / batch_size))\n",
        "            #val_loss_list.append(cumulative_val_loss/(val_feature_num / batch_size))\n",
        "            \n",
        "            val_loss_batch = cumulative_val_loss/(val_feature_num / batch_size)\n",
        "            if lowest_val_loss is None:\n",
        "                lowest_val_loss = val_loss\n",
        "            elif val_loss > lowest_val_loss - delta:\n",
        "                counter += 1\n",
        "                if counter >= patience:\n",
        "                    early_stop = True\n",
        "            else:\n",
        "                lowest_val_loss = val_loss\n",
        "                counter = 0\n",
        "\n",
        "\n",
        "            if t % 25 == 0:\n",
        "                plot_val_loss.append(val_loss.item() / num_tasks)\n",
        "                #print(val_loss.item() / num_tasks)\n",
        "                #print('   ', task_losses_val)\n",
        "\n",
        "                #print('   ', [math.exp(log_var) ** 0.5 for log_var in mtl.log_vars])\n",
        "            \n",
        "            if early_stop:\n",
        "                break\n",
        "        avg_min_val_loss[num_tasks - 2] += lowest_val_loss / num_tasks\n",
        "    avg_min_val_loss[num_tasks - 2] /= num_iters\n",
        "\n",
        "    print('Finished Task', num_tasks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgR_YOAAhWp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PV77UHYNzaM",
        "colab_type": "text"
      },
      "source": [
        "### Increase noise on task 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mLuxcXEMuoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import math\n",
        "torch.autograd.set_detect_anomaly(True) \n",
        "patience = 50\n",
        "delta = 1e-4\n",
        "num_iters = 3\n",
        "max_num_tasks = 5\n",
        "max_num_sigma = 5\n",
        "avg_min_val_loss = [0] * (max_num_sigma)\n",
        "\n",
        "for k in range(1, max_num_sigma+1):\n",
        "    num_tasks = 3\n",
        "    for i in range(num_iters):\n",
        "        print(i)\n",
        "        if fixed:\n",
        "            sigma = [fixed_sigma for _ in range(num_tasks)]\n",
        "        else:\n",
        "            #sigma = [random.randint(1, 3) for i in range(num_tasks)]\n",
        "            sigma = [1, 2, k]\n",
        "        X, Y = gen_polynomial_data(feature_num + val_feature_num, num_tasks, sigma, 8)\n",
        "        X_val, Y_val = X[feature_num:], [y[feature_num:] for y in Y]\n",
        "        X, Y = X[:feature_num], [y[:feature_num] for y in Y]\n",
        "        \n",
        "        lowest_val_loss = None\n",
        "        counter = 0\n",
        "        early_stop = False\n",
        "\n",
        "\n",
        "        train_data = Data(feature_num, X, Y)\n",
        "        val_data = Data(val_feature_num, X_val, Y_val)\n",
        "        train_data_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "        val_data_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "        model = MTLModel(hidden_dim, 1, num_tasks)\n",
        "        mtl = MultiTaskLossWrapper(num_tasks, model)\n",
        "\n",
        "        model.to(device)\n",
        "        mtl.to(device)\n",
        "\n",
        "        # https://github.com/keras-team/keras/blob/master/keras/optimizers.py\n",
        "        # k.epsilon() = keras.backend.epsilon()\n",
        "        optimizer = torch.optim.Adam(mtl.parameters(), lr=lr, eps=1e-07)\n",
        "\n",
        "        loss_list = []\n",
        "        val_loss_list = []\n",
        "        plot_val_loss = []\n",
        "        times = []\n",
        "        for t in range(nb_epoch):\n",
        "            start = time.time()\n",
        "            cumulative_loss = 0\n",
        "            cumulative_val_loss = 0\n",
        "            cumulative_task_losses = [0] * num_tasks\n",
        "            cumulative_task_losses_val = [0] * num_tasks\n",
        "\n",
        "            for X_batch, Y_batch in train_data_loader:\n",
        "                X_batch, Y_batch = X_batch.to(device), [y.to(device) for y in Y_batch]\n",
        "\n",
        "                loss, task_losses, log_vars = mtl(X_batch, Y_batch)\n",
        "                cumulative_task_losses = [cumulative_task_losses[i] + task_losses[i] for i in range(len(cumulative_task_losses))]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                cumulative_loss += loss.item()\n",
        "            with torch.no_grad():\n",
        "                for X_val_batch, Y_val_batch in val_data_loader:\n",
        "                    X_val_batch, Y_val_batch = X_val_batch.to(device), [y.to(device) for y in Y_val_batch]\n",
        "                    val_loss, task_losses_val, _ = mtl(X_val_batch, Y_val_batch)\n",
        "                    cumulative_task_losses_val = [cumulative_task_losses_val[i] + task_losses_val[i] for i in range(len(cumulative_task_losses_val))]\n",
        "\n",
        "                    cumulative_val_loss += val_loss.item()\n",
        "\n",
        "            #loss_list.append(cumulative_loss/(feature_num / batch_size))\n",
        "            #val_loss_list.append(cumulative_val_loss/(val_feature_num / batch_size))\n",
        "            \n",
        "            val_loss_batch = cumulative_val_loss/(val_feature_num / batch_size)\n",
        "            if lowest_val_loss is None:\n",
        "                lowest_val_loss = val_loss\n",
        "            elif val_loss > lowest_val_loss - delta:\n",
        "                counter += 1\n",
        "                if counter >= patience:\n",
        "                    early_stop = True\n",
        "            else:\n",
        "                lowest_val_loss = val_loss\n",
        "                counter = 0\n",
        "\n",
        "\n",
        "            if t % 25 == 0:\n",
        "                plot_val_loss.append(val_loss.item() / num_tasks)\n",
        "                #print(val_loss.item() / num_tasks)\n",
        "                #print('   ', task_losses_val)\n",
        "\n",
        "                #print('   ', [math.exp(log_var) ** 0.5 for log_var in mtl.log_vars])\n",
        "            \n",
        "            if early_stop:\n",
        "                break\n",
        "        avg_min_val_loss[k - 1] += lowest_val_loss / num_tasks\n",
        "    avg_min_val_loss[k - 1] /= num_iters\n",
        "\n",
        "    print('Finished Task', num_tasks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2uJbyYYOeeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}