{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multitask-mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aJoZ1G-S4t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "root_dir = \"/content/drive/My Drive/bayesian-multitask\"\n",
        "os.chdir(root_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98UOf0vXBDei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from mnist import MNIST\n",
        "from cifar import CIFAR10\n",
        "\n",
        "#num_tasks = 1\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Data(Dataset):\n",
        "\n",
        "    def __init__(self, feature_num, X, Y):\n",
        "        self.num_tasks = len(Y)\n",
        "        self.feature_num = feature_num\n",
        "\n",
        "        self.X = []\n",
        "        #self.X = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "        self.Y = []\n",
        "        for i in range(self.num_tasks):\n",
        "            self.X.append(0)\n",
        "            self.Y.append(0)\n",
        "        for i in range(self.num_tasks):\n",
        "            #self.Y[i] = torch.from_numpy(Y[i])\n",
        "            self.X[i] = torch.tensor(X[i], dtype=torch.float32, device=device)\n",
        "            self.Y[i] = torch.tensor(Y[i], dtype=torch.float32, device=device)\n",
        "    def __len__(self):\n",
        "        return self.feature_num\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[i][idx,:] for i in range(self.num_tasks)], [self.Y[i][idx,:] for i in range(self.num_tasks)]\n",
        "\n",
        "class MultiTaskLossWrapper(nn.Module):\n",
        "    def __init__(self, num_tasks, model, regression=True):\n",
        "        super(MultiTaskLossWrapper, self).__init__()\n",
        "        self.model = model\n",
        "        self.num_tasks = num_tasks\n",
        "        self.log_vars = nn.Parameter(torch.zeros((num_tasks), device=device))\n",
        "        self.regression = regression\n",
        "\n",
        "\n",
        "    def forward(self, input, targets, i):\n",
        "        #print(targets)\n",
        "        outputs = self.model(input)\n",
        "        loss = 0\n",
        "        task_losses = [0] * self.num_tasks\n",
        "        precision = [0] * self.num_tasks\n",
        "        if not self.regression:\n",
        "            loss_fn = nn.NLLLoss()\n",
        "        precision[i] = 0.5 * torch.exp(-self.log_vars[i])\n",
        "        if self.regression:\n",
        "            task_loss = torch.sum(precision[i] * (targets - outputs[i]) ** 2. + self.log_vars[i], -1)\n",
        "        else:\n",
        "            #print(outputs[i])\n",
        "            #print(targets[i])\n",
        "            #print(loss_fn(outputs[i], targets[i].long().squeeze()))\n",
        "            #print(precision[i] * loss_fn(outputs[i], targets[i].long().squeeze()) + self.log_vars[i])\n",
        "            task_loss = torch.sum(precision[i] * loss_fn(outputs[i], targets.long().squeeze()) + self.log_vars[i], -1)\n",
        "            #print(task_loss)\n",
        "            #print(1/0)\n",
        "        task_losses[i] = task_loss\n",
        "        loss += task_loss\n",
        "        \n",
        "        #loss = loss / self.num_tasks\n",
        "\n",
        "        return torch.mean(loss), task_losses, self.log_vars.data.tolist()\n",
        "\n",
        "\n",
        "class MTLModel(torch.nn.Module):\n",
        "    def __init__(self, num_tasks):\n",
        "        super(MTLModel, self).__init__()\n",
        "        self.num_tasks = num_tasks\n",
        "        self.shared_fc = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU())\n",
        "        self.nets = [0] * num_tasks\n",
        "        \n",
        "        for i in range(num_tasks):\n",
        "            self.nets[i] = nn.Sequential(nn.Linear(64, 10), nn.LogSoftmax(dim=1)).to(device)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        shared_out = self.shared_fc(x)\n",
        "        return [self.nets[i](shared_out) for i in range(self.num_tasks)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t32OssKJBM5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def gen_mnist_data(sigmas, datasets=['mnist', 'fashion']):\n",
        "    X = []\n",
        "    Y = []\n",
        "    X_test = []\n",
        "    Y_test = []\n",
        "    for idx, sigma in enumerate(sigmas):\n",
        "        if datasets[idx] == 'mnist':\n",
        "            mnist_train = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
        "                                              transform=torchvision.transforms.Compose([\n",
        "                                                  torchvision.transforms.ToTensor(),\n",
        "                                                  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                              ]))\n",
        "            mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True,\n",
        "                                              transform=torchvision.transforms.Compose([\n",
        "                                                  torchvision.transforms.ToTensor(),\n",
        "                                                  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                              ]))\n",
        "        elif datasets[idx] == 'fashion':\n",
        "            mnist_train = torchvision.datasets.FashionMNIST('./data', train=True, download=True,\n",
        "                                              transform=torchvision.transforms.Compose([\n",
        "                                                  torchvision.transforms.ToTensor(),\n",
        "                                                  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                              ]))\n",
        "            mnist_test = torchvision.datasets.FashionMNIST('./data', train=False, download=True,\n",
        "                                              transform=torchvision.transforms.Compose([\n",
        "                                                  torchvision.transforms.ToTensor(),\n",
        "                                                  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                              ]))\n",
        "        X.append(mnist_train.data.numpy())\n",
        "        y = mnist_train.targets.numpy()\n",
        "        X_test.append(mnist_test.data.numpy())\n",
        "        y_test = mnist_test.targets.numpy()\n",
        "        #Y.append(y[..., np.newaxis])\n",
        "        #Y_test.append(y_test[..., np.newaxis])\n",
        "    \n",
        "        y_noise = np.zeros_like(y)\n",
        "        y_test_noise = np.zeros_like(y_test)\n",
        "        for i in range(len(y)):\n",
        "            if abs(np.random.normal(0, sigma, 1)) > 2:\n",
        "                y_noise[i] = random.choice(list(range(0, y[i])) + list(range(y[i]+1, num_classes)))\n",
        "            else:\n",
        "                y_noise[i] = y[i]\n",
        "        for i in range(len(y_test)):\n",
        "            if abs(np.random.normal(0, 0, 1)) > 2:\n",
        "                y_test_noise[i] = random.choice(list(range(0, y_test[i])) + list(range(y_test[i]+1, num_classes)))\n",
        "            else:\n",
        "                y_test_noise[i] = y_test[i]\n",
        "        Y.append(y_noise[..., np.newaxis])\n",
        "        Y_test.append(y_test_noise[..., np.newaxis])\n",
        "    print('Done creating data...')\n",
        "    return X, Y, X_test, Y_test\n",
        "\n",
        "\n",
        "\n",
        "def calc_acc(model, train_data_loader, val_data_loader, epoch, num_tasks):\n",
        "    with torch.no_grad():\n",
        "        correct_train_list = [0] * num_tasks\n",
        "        total_train_list = [0] * num_tasks\n",
        "        correct_test_list = [0] * num_tasks\n",
        "        total_test_list = [0] * num_tasks\n",
        "\n",
        "        for X, Y in train_data_loader:\n",
        "            for i in range(num_tasks):\n",
        "                images, labels = X[i], Y[i]\n",
        "                images = images.to(device)\n",
        "            \n",
        "                train_outputs = model(images.reshape(batch_size, 784))\n",
        "                correct_train, total_train = 0, 0\n",
        "                labels = torch.flatten(labels.to(device))\n",
        "                #train_outputs = model.nets[0](model.shared_fc(images.reshape(100, 784)))\n",
        "                _, pred_train = torch.max(train_outputs[i], 1)\n",
        "                total_train = labels.shape[0]\n",
        "                correct_train = (pred_train == labels).sum().item()\n",
        "                correct_train_list[i] += correct_train\n",
        "                total_train_list[i] += total_train\n",
        "        for X_test, Y_test in val_data_loader:\n",
        "            for i in range(num_tasks):\n",
        "                images, labels = X_test[i], Y_test[i]\n",
        "                images = images.to(device)\n",
        "            \n",
        "                test_outputs = model(images.reshape(batch_size, 784))\n",
        "                #test_outputs = model.nets[0](model.shared_fc(images.reshape(100, 784)))\n",
        "                for i in range(num_tasks):\n",
        "                    correct_test, total_test = 0, 0\n",
        "                    labels = torch.flatten(labels.to(device))\n",
        "                    _, pred_test = torch.max(test_outputs[i], 1)\n",
        "                    total_test = labels.shape[0]\n",
        "                    correct_test = (pred_test == labels).sum().item()\n",
        "                    correct_test_list[i] += correct_test\n",
        "                    total_test_list[i] += total_test\n",
        "    \n",
        "    print('Train acc: ', [correct_train_list[i] / total_train_list[i] for i in range(num_tasks)])\n",
        "    print('   Val acc: ', [correct_test_list[i] / total_test_list[i] for i in range(num_tasks)])\n",
        "    return correct_train, total_train, correct_test, total_test\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0hLl-ujUgNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 41\n",
        "np.random.seed(seed)\n",
        "\n",
        "feature_num = 60000\n",
        "val_feature_num = 10000\n",
        "nb_epoch = 1000\n",
        "batch_size = 200\n",
        "hidden_dim = 512\n",
        "num_classes = 10\n",
        "lr = 0.00001\n",
        "patience = 1000\n",
        "delta = 1e-4\n",
        "max_num_tasks = 2\n",
        "\n",
        "random_sigma = False\n",
        "regression = False\n",
        "import random\n",
        "\n",
        "\n",
        "if random_sigma:\n",
        "    sigmas = [random.randint(1, 3) for _ in range(max_num_tasks)]\n",
        "else:\n",
        "    sigmas = [2 for _ in range(max_num_tasks)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LZWOd65Cr4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X, Y_data, X_val, Y_val_data = gen_mnist_data(sigma)\n",
        "X, Y_data, X_val, Y_val_data = gen_mnist_data(sigmas=[1,3], datasets=['mnist', 'mnist'])\n",
        "X = [x.reshape(60000, 784) for x in X]\n",
        "X_val = [x_val.reshape(10000, 784) for x_val in X_val]\n",
        "for num_tasks in range(2, max_num_tasks+1):\n",
        "    Y = Y_data[:num_tasks]\n",
        "    Y_val = Y_val_data[:num_tasks]\n",
        "    lowest_val_loss = None\n",
        "    counter = 0\n",
        "    early_stop = False\n",
        "\n",
        "    train_data = Data(feature_num, X, Y)\n",
        "    val_data = Data(val_feature_num, X_val, Y_val)\n",
        "    train_data_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "    val_data_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "    if regression:\n",
        "        model = MTLModel(num_tasks)\n",
        "        mtl = MultiTaskLossWrapper(num_tasks, model)\n",
        "    else:\n",
        "        model = MTLModel(num_tasks)\n",
        "        mtl = MultiTaskLossWrapper(num_tasks, model, regression=False)\n",
        "    \n",
        "    model.to(device)\n",
        "    mtl.to(device)\n",
        "\n",
        "    # https://github.com/keras-team/keras/blob/master/keras/optimizers.py\n",
        "    # k.epsilon() = keras.backend.epsilon()\n",
        "    optimizer = torch.optim.SGD(mtl.parameters(), lr=lr)\n",
        "\n",
        "    loss_list = []\n",
        "    val_loss_list = []\n",
        "    times = []\n",
        "    for t in range(nb_epoch):\n",
        "        cumulative_loss = 0.0\n",
        "        cumulative_val_loss = 0.0\n",
        "        cumulative_task_losses = [0] * num_tasks\n",
        "        cumulative_task_losses_val = [0] * num_tasks\n",
        "        loss = 0.0\n",
        "        for X_batch, Y_batch in train_data_loader:\n",
        "            for idx in range(num_tasks):\n",
        "                X_batch_idx, Y_batch_idx = X_batch[idx].to(device), Y_batch[idx].to(device)\n",
        "\n",
        "                task_loss, task_losses, log_vars = mtl(X_batch_idx, Y_batch_idx, idx)\n",
        "                cumulative_task_losses[idx] += task_losses[idx]\n",
        "                cumulative_loss += task_loss.item()\n",
        "                if idx == 0:\n",
        "                    loss = task_loss\n",
        "                else:\n",
        "                    loss += task_loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            for X_val_batch, Y_val_batch in val_data_loader:\n",
        "                for idx in range(num_tasks):\n",
        "                    X_val_batch_idx, Y_val_batch_idx = X_val_batch[idx].to(device), Y_val_batch[idx].to(device)\n",
        "                    val_task_loss, task_losses_val, _ = mtl(X_val_batch_idx, Y_val_batch_idx, idx)\n",
        "                    cumulative_task_losses_val[idx] += task_losses_val[idx]\n",
        "                    cumulative_val_loss += val_task_loss.item()\n",
        "\n",
        "        loss_list.append(cumulative_loss/(feature_num / batch_size))\n",
        "        val_loss_list.append(cumulative_val_loss/(val_feature_num / batch_size))\n",
        "        \n",
        "        val_loss_batch = cumulative_val_loss/(val_feature_num / batch_size)\n",
        "        if lowest_val_loss is None:\n",
        "            lowest_val_loss = val_loss_batch\n",
        "        elif val_loss_batch > lowest_val_loss - delta:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                early_stop = True\n",
        "        else:\n",
        "            lowest_val_loss = val_loss_batch\n",
        "            counter = 0\n",
        "        if t % 50 == 0:\n",
        "            correct_train, total_train, correct_test, total_test = calc_acc(model, train_data_loader, val_data_loader, t, num_tasks)\n",
        "            print('   Log vars: ', [math.exp(log_var) ** 0.5 for log_var in log_vars])\n",
        "        if early_stop:\n",
        "            print('Epochs:', t)\n",
        "            break\n",
        "    correct_train, total_train, correct_test, total_test = calc_acc(model, train_data_loader, val_data_loader, t, num_tasks)\n",
        "    pred_log_vars = [math.exp(log_var) ** 0.5 for log_var in log_vars]\n",
        "\n",
        "    print('Finished Task', num_tasks)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftvw7ViwTEtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}